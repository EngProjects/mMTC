{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS-AMP Simulations \n",
    "\n",
    "We will compare the error rate and runtime performance of the following three CCS cases: \n",
    "<ol>\n",
    "    <li>Classical Coded Compressed Sensing (CCS)</li>\n",
    "    <li>Coded Compressed Sensing with Message Passing on Factor Graph (CCS-Hybrid)</li>\n",
    "    <li>Coded Compressed Sensing with AMP (CCS-AMP), BP on outer graph</li>\n",
    "    <li>Coded Compressed Sensing with AMP (CCS-AMP), no BP on outer graph</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fht(u):\n",
    "    \"\"\"\n",
    "    Perform fast Hadamard transform of u, in-place.\n",
    "    Note len(u) must be a power of two.\n",
    "    \"\"\"\n",
    "    N = len(u)\n",
    "    i = N>>1\n",
    "    while i:\n",
    "        for j in range(N):\n",
    "            if (i&j) == 0:\n",
    "                temp = u[j]\n",
    "                u[j] += u[i|j]\n",
    "                u[i|j] = temp - u[i|j]\n",
    "        i>>= 1\n",
    "\n",
    "def sub_fht(n, m, seed=0, ordering=None, new_embedding=False):\n",
    "    \"\"\"\n",
    "    Returns functions to compute the sub-sampled Walsh-Hadamard transform,\n",
    "    i.e., operating with a wide rectangular matrix of random +/-1 entries.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns\n",
    "\n",
    "    It is most efficient (but not required) for max(m,n+1) to be a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional n-long array of row indices in [1, max(m,n)] to\n",
    "              implement subsampling; generated by seed if not specified,\n",
    "              but may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length m\n",
    "        Ay(y): computes A'.y (of length m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    if new_embedding:\n",
    "        w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "    else:\n",
    "        w = 2**int(np.ceil(np.log2(max(m, n+1))))\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (n,)\n",
    "    else:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        rng.shuffle(idxs)\n",
    "        ordering = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == m, \"x must be m long\"\n",
    "        y = np.zeros(w)\n",
    "        if new_embedding:\n",
    "            y[w-m:] = x.reshape(m)\n",
    "        else:\n",
    "            y[:m] = x.reshape(m)\n",
    "        fht(y)\n",
    "        return y[ordering]\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n, \"input must be n long\"\n",
    "        x = np.zeros(w)\n",
    "        x[ordering] = y.reshape(n)\n",
    "        fht(x)\n",
    "        if new_embedding:\n",
    "            return x[w-m:]\n",
    "        else:\n",
    "            return x[:m]\n",
    "\n",
    "    return Ax, Ay, ordering\n",
    "\n",
    "def block_sub_fht(n, m, l, seed=0, ordering=None, new_embedding=False):\n",
    "    \"\"\"\n",
    "    As `sub_fht`, but computes in `l` blocks of size `n` by `m`, potentially\n",
    "    offering substantial speed improvements.\n",
    "\n",
    "    n: number of rows\n",
    "    m: number of columns per block\n",
    "    l: number of blocks\n",
    "\n",
    "    It is most efficient (though not required) when max(m,n+1) is a power of 2.\n",
    "\n",
    "    seed: determines choice of random matrix\n",
    "    ordering: optional (l, n) shaped array of row indices in [1, max(m, n)] to\n",
    "              implement subsampling; generated by seed if not specified, but\n",
    "              may be given to speed up subsequent runs on the same matrix.\n",
    "\n",
    "    Returns (Ax, Ay, ordering):\n",
    "        Ax(x): computes A.x (of length n), with x having length l*m\n",
    "        Ay(y): computes A'.y (of length l*m), with y having length n\n",
    "        ordering: the ordering in use, which may have been generated from seed\n",
    "    \"\"\"\n",
    "    assert n > 0, \"n must be positive\"\n",
    "    assert m > 0, \"m must be positive\"\n",
    "    assert l > 0, \"l must be positive\"\n",
    "\n",
    "    if ordering is not None:\n",
    "        assert ordering.shape == (l, n)\n",
    "    else:\n",
    "        if new_embedding:\n",
    "            w = 2**int(np.ceil(np.log2(max(m+1, n+1))))\n",
    "        else:\n",
    "            w = 2**int(np.ceil(np.log2(max(m, n+1))))\n",
    "        rng = np.random.RandomState(seed)\n",
    "        ordering = np.empty((l, n), dtype=np.uint32)\n",
    "        idxs = np.arange(1, w, dtype=np.uint32)\n",
    "        for ll in range(l):\n",
    "            rng.shuffle(idxs)\n",
    "            ordering[ll] = idxs[:n]\n",
    "\n",
    "    def Ax(x):\n",
    "        assert x.size == l*m\n",
    "        out = np.zeros(n)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll],\n",
    "                                new_embedding=new_embedding)\n",
    "            out += ax(x[ll*m:(ll+1)*m])\n",
    "        return out\n",
    "\n",
    "    def Ay(y):\n",
    "        assert y.size == n\n",
    "        out = np.empty(l*m)\n",
    "        for ll in range(l):\n",
    "            ax, ay, _ = sub_fht(n, m, ordering=ordering[ll],\n",
    "                                new_embedding=new_embedding)\n",
    "            out[ll*m:(ll+1)*m] = ay(y)\n",
    "        return out\n",
    "\n",
    "    return Ax, Ay, ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Hadamard Transforms\n",
    "\n",
    "This code can all be found in `pyfht`, which uses a C extension to speed up the fht function. To make this notebook self contained, it's reproduced entirely in Python here, which will be quite slow!\n",
    "\n",
    "Skip to the next section if you're not interested in the specific transform implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfht import block_sub_fht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Tree encoder\n",
    "\n",
    "This function encodes the payloads corresponding to users into codewords from the specified tree code. \n",
    "\n",
    "Parity bits in section $i$ are generated based on the information sections $i$ is connected to\n",
    "\n",
    "Computations are done within the ring of integers modulo length of the section to enable FFT-based BP on the outer graph\n",
    "\n",
    "This function outputs the sparse representation of encoded messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree_encode(tx_message,K,messageBlocks,G,L,J):\n",
    "    encoded_tx_message = np.zeros((K,L),dtype=int)\n",
    "    \n",
    "    encoded_tx_message[:,0] = tx_message[:,0:J].dot(2**np.arange(J)[::-1])\n",
    "    for i in range(1,L):\n",
    "        if messageBlocks[i]:\n",
    "            # copy the message if i is an information section\n",
    "            encoded_tx_message[:,i] = tx_message[:,np.sum(messageBlocks[:i])*J:(np.sum(messageBlocks[:i])+1)*J].dot(2**np.arange(J)[::-1])\n",
    "        else:\n",
    "            # compute the parity if i is a parity section\n",
    "            indices = np.where(G[i])[0]\n",
    "            ParityInteger=np.zeros((K,1),dtype='int')\n",
    "            for j in indices:\n",
    "                ParityInteger1 = encoded_tx_message[:,j].reshape(-1,1)\n",
    "                ParityInteger = np.mod(ParityInteger+ParityInteger1,2**J)\n",
    "            encoded_tx_message[:,i] = ParityInteger.reshape(-1)\n",
    "    \n",
    "    return encoded_tx_message\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts message indices into $L$-sparse vectors of length $L 2^J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_indices_to_sparse(encoded_tx_message_indices,L,J,K):\n",
    "    encoded_tx_message_sparse=np.zeros((L*2**J,1),dtype=int)\n",
    "    for i in range(L):\n",
    "        A = encoded_tx_message_indices[:,i]\n",
    "        B = A.reshape([-1,1])\n",
    "        np.add.at(encoded_tx_message_sparse, i*2**J+B, 1)        \n",
    "    return encoded_tx_message_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the index representation corresponding to a SPARC-like vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_to_indices(cs_decoded_tx_message_sparse,L,J,listSize):\n",
    "    cs_decoded_tx_message = np.zeros((listSize,L),dtype=int)\n",
    "    for i in range(L):\n",
    "        A = cs_decoded_tx_message_sparse[i*2**J:(i+1)*2**J]\n",
    "        idx = (A.reshape(2**J,)).argsort()[np.arange(2**J-listSize)]\n",
    "        B = np.setdiff1d(np.arange(2**J),idx)\n",
    "        cs_decoded_tx_message[:,i] = B \n",
    "\n",
    "    return cs_decoded_tx_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information bits from retained paths in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_msg_indices(Paths,cs_decoded_tx_message, L,J):\n",
    "    msg_bits = np.empty(shape=(0,0))\n",
    "    L1 = Paths.shape[0]\n",
    "    for i in range(L1):\n",
    "        msg_bit=np.empty(shape=(0,0))\n",
    "        path = Paths[i].reshape(1,-1)\n",
    "        for j in range(path.shape[1]):\n",
    "            msg_bit = np.hstack((msg_bit,cs_decoded_tx_message[path[0,j],j].reshape(1,-1))) if msg_bit.size else cs_decoded_tx_message[path[0,j],j]\n",
    "            msg_bit=msg_bit.reshape(1,-1)\n",
    "        msg_bits = np.vstack((msg_bits,msg_bit)) if msg_bits.size else msg_bit           \n",
    "    return msg_bits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARC Codebook\n",
    "\n",
    "We use the `block_sub_fht` which computes the equivalent of $A.\\beta$ by using $L$ separate $M\\times M$ Hadamard matrices. However we want each entry to be divided by $\\sqrt{n}$ to get the right variance, and we need to do a reshape on the output to get column vectors, so we'll wrap those operations here.\n",
    "\n",
    "Returns two functions `Ab` and `Az` which compute $A\\cdot B$ and $z^T\\cdot A$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparc_codebook(L, M, n,P):\n",
    "    Ax, Ay, _ = block_sub_fht(n, M, L, ordering=None)\n",
    "    def Ab(b):\n",
    "        return Ax(b).reshape(-1, 1)/ np.sqrt(n)\n",
    "    def Az(z):\n",
    "        return Ay(z).reshape(-1, 1)/ np.sqrt(n) \n",
    "    return Ab, Az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP on outer graph\n",
    "\n",
    "This function computes the priors on the unknown sparse vector, given effective obervations of the (graph) neighboring sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePrior(s,G,messageBlocks,L,M,p0,K,τ,Phat,numBPiter,case):\n",
    "    \n",
    "    q = np.zeros(s.shape,dtype=float)\n",
    "    p1 = p0*np.ones(s.shape,dtype=float)\n",
    "    temp_beta = np.zeros((L*M, 1))\n",
    "    \n",
    "    for iter in range(numBPiter):\n",
    "        \n",
    "        # Translate the effective observation into PME. For the first iteration of BP, use the uninformative prior p0\n",
    "        if case==1:\n",
    "            for i in range(L):\n",
    "                temp_beta[i*M:(i+1)*M] = (p1[i*M:(i+1)*M]*np.exp(-(s[i*M:(i+1)*M]-np.sqrt(Phat))**2/(2*τ[i]**2)))/ \\\n",
    "                                         (p1[i*M:(i+1)*M]*np.exp(-(s[i*M:(i+1)*M]-np.sqrt(Phat))**2/(2*τ[i]**2)) + \\\n",
    "                                         (1-p1[i*M:(i+1)*M])*np.exp(-s[i*M:(i+1)*M]**2/(2*τ[i]**2))).astype(float) \\\n",
    "                                         .reshape(-1, 1)\n",
    "        else:\n",
    "            temp_beta = (p1*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)))/ (p1*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)) + (1-p1)*np.exp(-s**2/(2*τ**2))).astype(float).reshape(-1, 1)\n",
    "\n",
    "    \n",
    "        # Reshape PME into an LxM matrix\n",
    "        Beta = temp_beta.reshape(L,-1)\n",
    "        #print(Beta.shape,np.sum(Beta,axis=1))\n",
    "        Beta = Beta/(np.sum(Beta,axis=1).reshape(L,-1))\n",
    "        # Rotate PME 180deg about y-axis\n",
    "        Betaflipped = np.hstack((Beta[:,0].reshape(-1,1),np.flip(Beta[:,1:],axis=1)))\n",
    "        # Compute and store all FFTs\n",
    "        BetaFFT = np.fft.fft(Beta)\n",
    "        BetaflippedFFT = np.fft.fft(Betaflipped)\n",
    "        for i in range(L):\n",
    "            if messageBlocks[i]:\n",
    "                # Parity sections connected to info section i\n",
    "                parityIndices = np.where(G[i])[0]\n",
    "                BetaIFFTprime = np.empty((0,0)).astype(float)\n",
    "                for j in parityIndices:\n",
    "                    # Other info blocks connected to this parity block\n",
    "                    messageIndices = np.setdiff1d(np.where(G[j])[0],i)\n",
    "                    BetaFFTprime = np.vstack((BetaFFT[j],BetaflippedFFT[messageIndices,:]))\n",
    "                    # Multiply the relevant FFTs\n",
    "                    BetaFFTprime = np.prod(BetaFFTprime,axis=0)\n",
    "                    # IFFT\n",
    "                    BetaIFFTprime1 = np.fft.ifft(BetaFFTprime).real\n",
    "                    BetaIFFTprime = np.vstack((BetaIFFTprime,BetaIFFTprime1)) if BetaIFFTprime.size else BetaIFFTprime1\n",
    "                BetaIFFTprime = np.prod(BetaIFFTprime,axis=0)\n",
    "            else:\n",
    "                BetaIFFTprime = np.empty((0,0)).astype(float)\n",
    "                # Information sections connected to this parity section (assuming no parity over parity sections)\n",
    "                Indices = np.where(G[i])[0]\n",
    "                # FFT\n",
    "                BetaFFTprime = BetaFFT[Indices,:]\n",
    "                # Multiply the relevant FFTs\n",
    "                BetaFFTprime = np.prod(BetaFFTprime,axis=0)\n",
    "                # IFFT\n",
    "                BetaIFFTprime = np.fft.ifft(BetaFFTprime).real\n",
    "            \n",
    "            # Normalize to ensure it sums to one\n",
    "            p1[i*M:(i+1)*M] = (BetaIFFTprime/np.sum(BetaIFFTprime)).reshape(-1,1)\n",
    "            p1[i*M:(i+1)*M]  = 1-(1-p1[i*M:(i+1)*M] )**K \n",
    "            # Normalize to ensure sum of priors within a section is K (optional)\n",
    "            #p1[i*M:(i+1)*M] = p1[i*M:(i+1)*M]*K/np.sum(p1[i*M:(i+1)*M])\n",
    "         \n",
    "    q = np.minimum(p1,1)          \n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMP\n",
    "This is the actual AMP algorithm. It's a mostly straightforward transcription from the relevant equations, but note we use `longdouble` types because the expentials are often too big to fit into a normal `double`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amp(y, σ_n, P, L, M, T, Ab, Az,p0,K,G,messageBlocks,BPonOuterGraph,numBPiter,case):\n",
    "\n",
    "    # set up AMP parameters\n",
    "    n = y.size\n",
    "    β = np.zeros((L*M, 1))\n",
    "    s = np.zeros((L*M, 1))\n",
    "    z = y.copy()\n",
    "    Phat = n*P/L\n",
    "    τ_evolution = np.zeros((T,1))\n",
    "    \n",
    "    # set up case-specific parameters\n",
    "    if case != 2:\n",
    "        τ = np.zeros((L, 1))\n",
    "    \n",
    "    # begin AMP iterations\n",
    "    for t in range(T):\n",
    "        \n",
    "        # Begin case-specific code\n",
    "        \n",
    "        # CCS (L independent instances of AMP)\n",
    "        if case==0:\n",
    "            # set up case-0 specific parameters\n",
    "            numBlockRows = n//L\n",
    "            \n",
    "            # iterate through each of L independent instances of AMP\n",
    "            for i in range(L):\n",
    "                # Compute τ online using the residual\n",
    "                τ[i] = np.sqrt(np.sum(z[i*numBlockRows:(i+1)*numBlockRows]**2)/numBlockRows)\n",
    "            \n",
    "                # compute effective observation\n",
    "                Azz = Az(z[i*numBlockRows:(i+1)*numBlockRows]).astype(np.longdouble)\n",
    "                s[i*M:(i+1)*M] = (np.sqrt(Phat)*β[i*M:(i+1)*M] + Azz).astype(np.longdouble)\n",
    "                \n",
    "                # use uninformative prior\n",
    "                q = p0\n",
    "                \n",
    "                # denoiser\n",
    "                β[i*M:(i+1)*M] = (q*np.exp(-(s[i*M:(i+1)*M]-np.sqrt(Phat))**2/(2*τ[i]**2)))/ \\\n",
    "                                 (q*np.exp(-(s[i*M:(i+1)*M]-np.sqrt(Phat))**2/(2*τ[i]**2)) + (1-q)*np.exp(-s[i*M:(i+1)*M]**2/(2*τ[i]**2))) \\\n",
    "                                 .astype(float).reshape(-1, 1)\n",
    "                \n",
    "                # residual\n",
    "                z[i*numBlockRows:(i+1)*numBlockRows] = y[i*numBlockRows:(i+1)*numBlockRows] - np.sqrt(Phat)*Ab(β[i*M:(i+1)*M]) + \\\n",
    "                                                       (z[i*numBlockRows:(i+1)*numBlockRows]/(numBlockRows*τ[i]**2)) * \\\n",
    "                                                       (Phat*np.sum(β[i*M:(i+1)*M]) - Phat*np.sum(β[i*M:(i+1)*M]**2))\n",
    "            # store value for tau_evolution\n",
    "            τ_evolution[t] = τ[0]\n",
    "            \n",
    "            # end of case-0 specific code\n",
    "            \n",
    "        # CCS-Hybrid\n",
    "        elif case==1:\n",
    "            # set up case-1 specific parameters\n",
    "            numBlockRows = n//L\n",
    "            \n",
    "            # iterate through each of L independent instances of AMP\n",
    "            for i in range(L):\n",
    "                # Compute τ online using the residual\n",
    "                τ[i] = np.sqrt(np.sum(z[i*numBlockRows:(i+1)*numBlockRows]**2)/numBlockRows)\n",
    "            \n",
    "                # compute effective observation\n",
    "                Azz = Az(z[i*numBlockRows:(i+1)*numBlockRows]).astype(np.longdouble)\n",
    "                s[i*M:(i+1)*M] = (np.sqrt(Phat)*β[i*M:(i+1)*M] + Azz).astype(np.longdouble)\n",
    "                \n",
    "            # compute priors tying together all L instances of AMP\n",
    "            q = computePrior(s,G,messageBlocks,L,M,p0,K,τ,Phat,numBPiter,case)\n",
    "                \n",
    "            # iterate through each of L independent instances of AMP\n",
    "            for i in range(L):\n",
    "                # denoiser\n",
    "                β[i*M:(i+1)*M] = (q[i*M:(i+1)*M]*np.exp(-(s[i*M:(i+1)*M]-np.sqrt(Phat))**2/(2*τ[i]**2)))/ \\\n",
    "                                 (q[i*M:(i+1)*M]*np.exp(-(s[i*M:(i+1)*M]-np.sqrt(Phat))**2/(2*τ[i]**2)) + \\\n",
    "                                 (1-q[i*M:(i+1)*M])*np.exp(-s[i*M:(i+1)*M]**2/(2*τ[i]**2))).astype(float).reshape(-1, 1)\n",
    "                \n",
    "                # residual\n",
    "                z[i*numBlockRows:(i+1)*numBlockRows] = y[i*numBlockRows:(i+1)*numBlockRows] - np.sqrt(Phat)*Ab(β[i*M:(i+1)*M]) + \\\n",
    "                                                       (z[i*numBlockRows:(i+1)*numBlockRows]/(numBlockRows*τ[i]**2)) * \\\n",
    "                                                       (Phat*np.sum(β[i*M:(i+1)*M]) - Phat*np.sum(β[i*M:(i+1)*M]**2))\n",
    "            # store value for tau_evolution\n",
    "            τ_evolution[t] = τ[0]\n",
    "            \n",
    "            # end of case-1 specific code\n",
    "            \n",
    "        # CCS-AMP with and without BP on outer graph\n",
    "        elif case==2 or case==3:\n",
    "            # Compute τ online using the residual\n",
    "            τ = np.sqrt(np.sum(z**2)/n)\n",
    "\n",
    "            # effective observation\n",
    "            s = (np.sqrt(Phat)*β + Az(z)).astype(np.longdouble)\n",
    "\n",
    "            if BPonOuterGraph==0:\n",
    "                # Use the uninformative prior p0 for Giuseppe's scheme\n",
    "                q = p0\n",
    "            else:\n",
    "                # Compute the prior through BP on outer graph\n",
    "                q = computePrior(s,G,messageBlocks,L,M,p0,K,τ,Phat,numBPiter,case)\n",
    "\n",
    "            # denoiser\n",
    "            β = (q*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)))/ (q*np.exp(-(s-np.sqrt(Phat))**2/(2*τ**2)) + (1-q)*np.exp(-s**2/(2*τ**2))).astype(float).reshape(-1, 1)\n",
    "\n",
    "            # residual\n",
    "            z = y - np.sqrt(Phat)*Ab(β) + (z/(n*τ**2)) * (Phat*np.sum(β) - Phat*np.sum(β**2))\n",
    "            \n",
    "            # update tau_evolution\n",
    "            τ_evolution[t] = τ\n",
    "            \n",
    "            # end of case-2 specific code\n",
    "        \n",
    "        # End case-specific code\n",
    "        \n",
    "\n",
    "    return β, τ_evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outer Tree decoder\n",
    "\n",
    "This function implements the tree deocoder for a specific graph corresponding to the outer tree code\n",
    "\n",
    "It is currently hard-coded for a specfic architecture\n",
    "\n",
    "The architecture is based on a tri-adic design and can be found in the simulation results section of https://arxiv.org/pdf/2001.03705.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree_decoder(cs_decoded_tx_message,G,L,J,B,listSize):\n",
    "    \n",
    "    tree_decoded_tx_message = np.empty(shape=(0,0))\n",
    "    \n",
    "    Paths012 = merge_paths(cs_decoded_tx_message[:,0:3])\n",
    "    \n",
    "    Paths345 = merge_paths(cs_decoded_tx_message[:,3:6])\n",
    "    \n",
    "    Paths678 = merge_paths(cs_decoded_tx_message[:,6:9])\n",
    "    \n",
    "    Paths91011 = merge_paths(cs_decoded_tx_message[:,9:12])\n",
    "    \n",
    "    Paths01267812 = merge_pathslevel2(Paths012,Paths678,cs_decoded_tx_message[:,[0,6,12]])\n",
    "    \n",
    "    Paths3459101113 = merge_pathslevel2(Paths345,Paths91011,cs_decoded_tx_message[:,[3,9,13]])\n",
    "    \n",
    "    Paths01267812345910111314 = merge_all_paths0(Paths01267812,Paths3459101113,cs_decoded_tx_message[:,[1,4,10,14]])\n",
    "    \n",
    "    Paths = merge_all_paths_final(Paths01267812345910111314,cs_decoded_tx_message[:,[7,10,15]])\n",
    "    \n",
    "    \n",
    "   \n",
    "    return Paths\n",
    "\n",
    "def merge_paths(A):\n",
    "    listSize = A.shape[0]\n",
    "    B = np.array([np.mod(A[:,0] + a,2**16) for a in A[:,1]]).flatten()\n",
    "     \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,listSize).reshape(-1,1),np.floor(I/listSize).reshape(-1,1)]).astype(int)\n",
    "            Paths = np.vstack((Paths,np.hstack([I1,np.repeat(i,I.shape[0]).reshape(-1,1)]))) if Paths.size else np.hstack([I1,np.repeat(i,I.shape[0]).reshape(-1,1)])\n",
    "    \n",
    "    return Paths\n",
    "\n",
    "def merge_pathslevel2(Paths012,Paths678,A):\n",
    "    listSize = A.shape[0]\n",
    "    Paths0 = Paths012[:,0]\n",
    "    Paths6 = Paths678[:,0]\n",
    "    B = np.array([np.mod(A[Paths0,0] + a,2**16) for a in A[Paths6,1]]).flatten()\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,Paths0.shape[0]).reshape(-1,1),np.floor(I/Paths0.shape[0]).reshape(-1,1)]).astype(int)\n",
    "            PPaths = np.hstack((Paths012[I1[:,0]].reshape(-1,3),Paths678[I1[:,1]].reshape(-1,3),np.repeat(i,I1.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "               \n",
    "    return Paths\n",
    "\n",
    "\n",
    "def merge_all_paths0(Paths01267812,Paths3459101113,A):\n",
    "    listSize = A.shape[0]\n",
    "    Paths1 = Paths01267812[:,1]\n",
    "    Paths4 = Paths3459101113[:,1]\n",
    "    Paths10 = Paths3459101113[:,4]\n",
    "    Aa = np.mod(A[Paths4,1]+A[Paths10,2],2**16)\n",
    "    B = np.array([np.mod(A[Paths1,0] + a,2**16) for a in Aa]).flatten()\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,3])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            I1 = np.hstack([np.mod(I,Paths1.shape[0]).reshape(-1,1),np.floor(I/Paths1.shape[0]).reshape(-1,1)]).astype(int)\n",
    "            PPaths = np.hstack((Paths01267812[I1[:,0]].reshape(-1,7),Paths3459101113[I1[:,1]].reshape(-1,7),np.repeat(i,I1.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "    \n",
    "    return Paths\n",
    "\n",
    "def merge_all_paths_final(Paths01267812345910111314,A):\n",
    "    \n",
    "    listSize = A.shape[0]\n",
    "    Paths7 = Paths01267812345910111314[:,4]\n",
    "    Paths10 = Paths01267812345910111314[:,11]\n",
    "    B = np.mod(A[Paths7,0] + A[Paths10,1] ,2**16)\n",
    "    \n",
    "    Paths=np.empty((0,0))\n",
    "    \n",
    "    for i in range(listSize):\n",
    "        I = np.where(B==A[i,2])[0].reshape(-1,1)\n",
    "        if I.size:\n",
    "            PPaths = np.hstack((Paths01267812345910111314[I].reshape(-1,15),np.repeat(i,I.shape[0]).reshape(-1,1)))\n",
    "            Paths = np.vstack((Paths,PPaths)) if Paths.size else PPaths\n",
    "    return Paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If tree decoder outputs more than $K$ valid paths, retain $K-\\delta$ of them based on their LLRs\n",
    "\n",
    "$\\delta$ is currently set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_topKminusdelta_paths(Paths, cs_decoded_tx_message, β, J,K,delta):\n",
    "    \n",
    "    L1 = Paths.shape[0]\n",
    "    LogL = np.zeros((L1,1))\n",
    "    for i in range(L1):\n",
    "        msg_bit=np.empty(shape=(0,0))\n",
    "        path = Paths[i].reshape(1,-1)\n",
    "        for j in range(path.shape[1]):\n",
    "            msg_bit = np.hstack((msg_bit,j*(2**J)+cs_decoded_tx_message[path[0,j],j].reshape(1,-1))) if msg_bit.size else j*(2**J)+cs_decoded_tx_message[path[0,j],j]\n",
    "            msg_bit=msg_bit.reshape(1,-1)\n",
    "        LogL[i] = np.sum(np.log(β[msg_bit])) \n",
    "    Indices =  LogL.reshape(1,-1).argsort()[0,-(K-delta):]\n",
    "    Paths = Paths[Indices,:].reshape(((K-delta),-1))\n",
    "    \n",
    "    return Paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=100 # Number of active users\n",
    "B=128 # Payload size of each active user\n",
    "L=16 # Number of sections/sub-blocks\n",
    "n=38400 # Total number of channel uses (real d.o.f)\n",
    "T=10 # Number of AMP iterations\n",
    "listSize = K+10  # List size retained for each section after AMP converges\n",
    "J=16  # Length of each coded sub-block\n",
    "M=2**J # Length of each section\n",
    "messageBlocks = np.array([1,1,0,1,1,0,1,1,0,1,1,0,0,0,0,0]).astype(int) # Indicates the indices of information blocks\n",
    "# Adjacency matrix of the outer code/graph\n",
    "G = np.zeros((L,L)).astype(int)\n",
    "# G contains info on what parity blocks a message is attached to and what message blocks a parity is involved with\n",
    "# Currently, we do not allow parity over parities. BP code needs to be modified a little to accomodate parity over parities\n",
    "G[0,[2,12]]=1\n",
    "G[1,[2,14]]=1\n",
    "G[2,[0,1]]=1\n",
    "G[3,[5,13]]=1\n",
    "G[4,[5,14]]=1\n",
    "G[5,[3,4]]=1\n",
    "G[6,[8,12]]=1\n",
    "G[7,[8,15]]=1\n",
    "G[8,[6,7]]=1\n",
    "G[9,[11,13]]=1\n",
    "G[10,[11,14,15]]=1\n",
    "G[11,[9,10]]=1\n",
    "G[12,[0,6]]=1\n",
    "G[13,[3,9]]=1\n",
    "G[14,[1,4,10]]=1\n",
    "G[15,[7,10]]=1\n",
    "BPonOuterGraph = 1 # Indicates if BP is allowed on the outer code.Setting this to zero defaults back to Giuseppe's scheme that uses uninformative prior\n",
    "numBPiter = 1; # Number of BP iterations on outer code. 1 seems to be good enough & AMP theory including state evolution valid only for one BP iteration\n",
    "p0 = 1-(1-1/M)**K # Giuseppe's uninformative prior\n",
    "delta = 0\n",
    "maxSims=100 # number of simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(EbNodB, case):\n",
    "    # EbN0 in linear scale\n",
    "    EbNo = 10**(EbNodB/10)\n",
    "    P = 2*B*EbNo/n\n",
    "    σ_n = 1\n",
    "    \n",
    "    # Power estimate\n",
    "    Phat = n*P/L\n",
    "    \n",
    "    # variables used for measuring algorithm performance\n",
    "    msgDetected = 0\n",
    "    avgTime = 0\n",
    "    \n",
    "    # run simulation maxSims times\n",
    "    for sims in range(maxSims):\n",
    "        \n",
    "        # Generate active users message sequences\n",
    "        tx_message = np.random.randint(2, size=(K,B))\n",
    "\n",
    "        # Outer-encode the message sequences\n",
    "        encoded_tx_message_indices = Tree_encode(tx_message,K,messageBlocks,G,L,J)\n",
    "\n",
    "        # Convert indices to sparse representation\n",
    "        β_0 = convert_indices_to_sparse(encoded_tx_message_indices,L,J,K)\n",
    "        \n",
    "        # Begin case-specific code\n",
    "        \n",
    "        # CCS\n",
    "        if case==0:\n",
    "            # set up case 0\n",
    "            assert n % L == 0\n",
    "            numBlockRows = n//L\n",
    "            \n",
    "            # create Ab, Az matricies\n",
    "            Ab, Az = sparc_codebook(1, M, numBlockRows, P)\n",
    "            \n",
    "            # obtain compressed signal to transmit through channel\n",
    "            x = np.zeros((n, 1))\n",
    "            for i in range(L):\n",
    "                x[i*numBlockRows:(i+1)*numBlockRows] = np.sqrt(Phat)*Ab(β_0[i*M:(i+1)*M])\n",
    "            \n",
    "            # generate random channel noise and received signal\n",
    "            z = np.random.randn(n, 1) * σ_n\n",
    "            y = (x + z).reshape(-1, 1)\n",
    "            \n",
    "            # set up AMP data structure\n",
    "            β = np.zeros((L*M, 1))\n",
    "            \n",
    "            # start timer\n",
    "            tic = time.time()\n",
    "            \n",
    "            # run CS on individual blocks using AMP.  (L independent instances of AMP)\n",
    "            β, τ_evolution = amp(y, σ_n, P, L, M, T, Ab, Az,p0,K,G,messageBlocks,BPonOuterGraph,numBPiter,case)\n",
    "            \n",
    "            # stop timer\n",
    "            toc = time.time()\n",
    "            \n",
    "            # end case-0 specific code\n",
    "        \n",
    "        # CCS-Hybrid\n",
    "        elif case==1:\n",
    "            # set up case 1\n",
    "            assert n % L == 0\n",
    "            numBlockRows = n//L\n",
    "            \n",
    "            # create Ab, Az matricies\n",
    "            Ab, Az = sparc_codebook(1, M, numBlockRows, P)\n",
    "            \n",
    "            # obtain compressed signal to transmit through channel\n",
    "            x = np.zeros((n, 1))\n",
    "            for i in range(L):\n",
    "                x[i*numBlockRows:(i+1)*numBlockRows] = np.sqrt(Phat)*Ab(β_0[i*M:(i+1)*M])\n",
    "            \n",
    "            # generate random channel noise and received signal\n",
    "            z = np.random.randn(n, 1) * σ_n\n",
    "            y = (x + z).reshape(-1, 1)\n",
    "            \n",
    "            # set up AMP data structure\n",
    "            β = np.zeros((L*M, 1))\n",
    "            \n",
    "            # start timer\n",
    "            tic = time.time()\n",
    "            \n",
    "            # run CS on individual blocks using AMP.  (L independent instances of AMP tied together by BP)\n",
    "            β, τ_evolution = amp(y, σ_n, P, L, M, T, Ab, Az,p0,K,G,messageBlocks,BPonOuterGraph,numBPiter,case)\n",
    "            \n",
    "            # stop timer\n",
    "            toc = time.time()\n",
    "            \n",
    "            # end case-1 specific code\n",
    "            \n",
    "        # CCS-AMP with BP on Outer Graph\n",
    "        elif case==2:\n",
    "            \n",
    "            # Set BPonOuterGraph = 1\n",
    "            BPonOuterGraph = 1            \n",
    "            \n",
    "            # Generate the binned SPARC codebook\n",
    "            Ab, Az = sparc_codebook(L, M, n, P)\n",
    "\n",
    "            # Generate our transmitted signal X\n",
    "            x = np.sqrt(Phat)*Ab(β_0)\n",
    "\n",
    "            # Generate random channel noise and thus also received signal y\n",
    "            z = np.random.randn(n, 1) * σ_n\n",
    "            y = (x + z).reshape(-1, 1)\n",
    "            \n",
    "            # start timer\n",
    "            tic = time.time()\n",
    "            \n",
    "            # Run AMP decoding\n",
    "            β, τ_evolution = amp(y, σ_n, P, L, M, T, Ab, Az,p0,K,G,messageBlocks,BPonOuterGraph,numBPiter,case)\n",
    "            \n",
    "            # stop timer\n",
    "            toc = time.time()\n",
    "            \n",
    "            # end case-2 specific code\n",
    "            \n",
    "        # CCS-AMP without BP on Outer Graph\n",
    "        elif case==3:\n",
    "            \n",
    "            # Set BPonOuterGraph = 0\n",
    "            BPonOuterGraph = 0\n",
    "            \n",
    "            # Generate the binned SPARC codebook\n",
    "            Ab, Az = sparc_codebook(L, M, n, P)\n",
    "\n",
    "            # Generate our transmitted signal X\n",
    "            x = np.sqrt(Phat)*Ab(β_0)\n",
    "\n",
    "            # Generate random channel noise and thus also received signal y\n",
    "            z = np.random.randn(n, 1) * σ_n\n",
    "            y = (x + z).reshape(-1, 1)\n",
    "            \n",
    "            # start timer\n",
    "            tic = time.time()\n",
    "            \n",
    "            # Run AMP decoding\n",
    "            β, τ_evolution = amp(y, σ_n, P, L, M, T, Ab, Az,p0,K,G,messageBlocks,BPonOuterGraph,numBPiter,case)\n",
    "            \n",
    "            # stop timer\n",
    "            toc = time.time()\n",
    "            \n",
    "            # end case-2 specific code\n",
    "        else:\n",
    "            raise Exception('Invalid case')\n",
    "        \n",
    "        # End case-specific code\n",
    "        \n",
    "\n",
    "        # Convert decoded sparse vector into vector of indices  \n",
    "        cs_decoded_tx_message = convert_sparse_to_indices(β,L,J,listSize)\n",
    "\n",
    "        # Tree decoder to decode individual messages from lists output by AMP\n",
    "        Paths = Tree_decoder(cs_decoded_tx_message,G,L,J,B,listSize)\n",
    "\n",
    "        # Re-align paths to the correct order\n",
    "        perm = np.argsort(np.array([0,1,2,6,7,8,12,3,4,5,9,10,11,13,14,15]))\n",
    "        Paths = Paths[:,perm]\n",
    "\n",
    "        # If tree deocder outputs more than K valid paths, retain only K of them\n",
    "        if Paths.shape[0] > K:\n",
    "            Paths = pick_topKminusdelta_paths(Paths, cs_decoded_tx_message, β, J, K,0)\n",
    "\n",
    "        # Extract the message indices from valid paths in the tree    \n",
    "        Tree_decoded_indices = extract_msg_indices(Paths,cs_decoded_tx_message, L,J)\n",
    "\n",
    "        # Calculation of per-user prob err\n",
    "        for i in range(K):\n",
    "            msgDetected = msgDetected + np.equal(encoded_tx_message_indices[i,:],Tree_decoded_indices).all(axis=1).any()\n",
    "\n",
    "        # update avgTime\n",
    "        avgTime += (toc - tic)\n",
    "    \n",
    "    # compute error rate and average time duration\n",
    "    errorRate = (K*maxSims - msgDetected)/(K*maxSims)\n",
    "    avgTime /= maxSims\n",
    "    \n",
    "    return errorRate, avgTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 0: CCS\n",
    "# Case 1: CCS-Hybrid\n",
    "# Case 2: CCS-AMP, BP on Outer Graph\n",
    "# Case 3: CCS-AMP, No BP on Outer Graph\n",
    "\n",
    "numCases = 4\n",
    "SNR = np.array([1, 1.5, 2, 2.5, 3, 3.5, 4])\n",
    "results = np.zeros((numCases, len(SNR)))\n",
    "times = np.zeros((numCases, len(SNR)))\n",
    "\n",
    "# Notify commencement of simulation\n",
    "print('***Starting Simulations*****')\n",
    "\n",
    "# Compare full A vs block diagonal A\n",
    "for case in range(numCases):\n",
    "    for idxSnr in range(len(SNR)):\n",
    "        print(f'Running simulation {case*len(SNR)+idxSnr+1}/{numCases*len(SNR)}')\n",
    "        results[case, idxSnr], times[case, idxSnr] = simulate(SNR[idxSnr], case)\n",
    "\n",
    "# Notify completion\n",
    "print('*****All Simulations Complete*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error rate results in linear scale\n",
    "plt.figure(1)\n",
    "plt.plot(SNR, results[0, :], 'b', label=\"Block Diagonal A, No BP on Outer Graph\")\n",
    "plt.plot(SNR, results[1, :], 'r', label=\"Block Diagonal A, BP on Outer Graph\")\n",
    "plt.plot(SNR, results[2, :], 'k', label=\"CCS-AMP, BP on Outer Graph\")\n",
    "plt.plot(SNR, results[3, :], 'g', label=\"CCS-AMP, No BP on Outer Graph\")\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\frac{E_b}{N_0}$')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title(r'Error Rates vs $\\frac{E_b}{N_0}$')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot error rate results in logarithmic scale\n",
    "plt.figure(2)\n",
    "plt.semilogy(SNR, results[0, :], 'b', label=\"Block Diagonal A, No BP on Outer Graph\")\n",
    "plt.semilogy(SNR, results[1, :], 'r', label=\"Block Diagonal A, BP on Outer Graph\")\n",
    "plt.semilogy(SNR, results[2, :], 'k', label=\"CCS-AMP, BP on Outer Graph\")\n",
    "plt.semilogy(SNR, results[3, :], 'g', label=\"CCS-AMP, No BP on Outer Graph\")\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\frac{E_b}{N_0}$')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title(r'Error Rates vs $\\frac{E_b}{N_0}$')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot time results\n",
    "plt.figure(3)\n",
    "plt.plot(SNR, times[0, :], 'b', label=\"Block Diagonal A, No BP on Outer Graph\")\n",
    "plt.plot(SNR, times[1, :], 'r', label=\"Block Diagonal A, BP on Outer Graph\")\n",
    "plt.plot(SNR, times[2, :], 'k', label=\"CCS-AMP, BP on Outer Graph\")\n",
    "plt.plot(SNR, times[3, :], 'g', label=\"CCS-AMP, No BP on Outer Graph\")\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\frac{E_b}{N_0}$')\n",
    "plt.ylabel(r'Average Runtime (seconds)')\n",
    "plt.title(r'Average Runtime vs $\\frac{E_b}{N_0}$')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error rates: ')\n",
    "print(results)\n",
    "print('Times: ')\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
